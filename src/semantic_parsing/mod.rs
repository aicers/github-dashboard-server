#![allow(clippy::uninlined_format_args)]

use anyhow::Result;
use langchain_rust::{language_models::llm::LLM, llm::client::Ollama, schemas::Message};

pub(crate) async fn invoke(schema: &str, prompt: &str) -> Result<String> {
    let today = chrono::Utc::now().format("%Y-%m-%d").to_string();

    let system_msg = Message::new_system_message(format!(
        "You are a helpful assistant that translates natural language into GraphQL queries.\n\n\
        A GraphQL schema will be provided with the following background:\n\n\
        - It was generated by the AICE GitHub Dashboard Server.\n\n\
        - The server collects issues and pull requests created in the 'aicers' GitHub organization.\n\n\
        - The user querying the schema is a member of the 'aicers' GitHub organization and wants to know statistics.\n\n\
        There are some rules you must follow:\n\n\
        - Return `{{}}` if the answer cannot be found in the schema.\n\n\
        - Return a GraphQL query that answers the natural language query based on the schema.\n\n\
        - Don't make up an answer if one cannot be found.\n\n\
        - Don't use any queries that return a type ending in `Connection!`.\n\n\
        - Don't explain the query, just return it.\n\n\
        - If an answer is found, return it in the format `query {{ ... }}` or `{{}}`.\n\n\
        - Today's date is {}.\n\n\
        - Timezone: UTC.\n\n\
        Schema:\n{}\n\n\
        ",
        today, schema
    ));
    let human_msg = Message::new_human_message(prompt);
    let messages = vec![system_msg, human_msg];

    // let ollama = Ollama::default().with_model("deepseek-r1:8b");
    let ollama = Ollama::default().with_model("llama3.2");
    Ok(ollama.generate(&messages).await?.generation)
}
